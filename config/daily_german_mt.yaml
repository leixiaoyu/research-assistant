# Daily German MT Research Configuration
# ========================================
#
# Optimized for automated daily runs via launchd.
# Focus: German machine translation challenges
#
# Topics covered:
#   1. Formality and Register (Sie/du, politeness)
#   2. Long-form and Literary Translation
#   3. Prompt Engineering for Translation
#   4. Context-Aware Translation
#   5. Style Preservation in Translation
#
# Cost estimate: ~$0.10-0.50/day with Gemini Flash
# Execution time: ~15-30 minutes
#
# Usage:
#   python -m src.cli run --config config/daily_german_mt.yaml

research_topics:
  # -------------------------------------------------------------------------
  # Topic 1: Formality and Register
  # -------------------------------------------------------------------------
  # German has distinct formal (Sie) and informal (du) address forms.
  # This topic tracks research on handling register/formality in MT.
  - query: 'neural machine translation low-resource language'
    provider: "semantic_scholar"
    auto_select_provider: false  # Force Semantic Scholar
    timeframe:
      type: "recent"
      value: "30d"
    max_papers: 20
    extraction_targets:
      - name: "engineering_summary"
        description: "2-paragraph summary focusing on practical applications for handling German formality in translation systems"
        output_format: "text"
        required: true

  # -------------------------------------------------------------------------
  # Topic 2: Long-form and Literary Translation
  # -------------------------------------------------------------------------
  # Challenges in translating long documents: coherence, consistency,
  # literary style preservation, discourse-level phenomena.
  - query: 'document-level machine translation German'
    provider: "semantic_scholar"
    auto_select_provider: false  # Force Semantic Scholar
    timeframe:
      type: "recent"
      value: "30d"
    max_papers: 20
    extraction_targets:
      - name: "engineering_summary"
        description: "Focus on techniques for handling long context, maintaining coherence, and preserving literary style in German translation"
        output_format: "text"
        required: true

  # -------------------------------------------------------------------------
  # Topic 3: Prompt Engineering for Translation
  # -------------------------------------------------------------------------
  # Using prompts and in-context learning to improve translation quality,
  # especially for German-specific challenges.
  - query: 'prompt engineering machine translation multilingual'
    provider: "semantic_scholar"
    auto_select_provider: false  # Force Semantic Scholar
    timeframe:
      type: "recent"
      value: "30d"
    max_papers: 25
    extraction_targets:
      - name: "engineering_summary"
        description: "Focus on prompt design patterns, few-shot examples, and techniques for improving German translation quality through prompting"
        output_format: "text"
        required: true

  # -------------------------------------------------------------------------
  # Topic 4: Context-Aware Translation
  # -------------------------------------------------------------------------
  # Using context beyond the sentence level: document context,
  # discourse markers, anaphora resolution, topic consistency.
  - query: 'context-aware neural machine translation discourse'
    provider: "semantic_scholar"
    auto_select_provider: false  # Force Semantic Scholar
    timeframe:
      type: "recent"
      value: "30d"
    max_papers: 20
    extraction_targets:
      - name: "engineering_summary"
        description: "Focus on context modeling techniques, discourse-aware translation, and handling German discourse phenomena"
        output_format: "text"
        required: true

  # -------------------------------------------------------------------------
  # Topic 5: Style Preservation in Translation
  # -------------------------------------------------------------------------
  # Maintaining authorial voice, tone, and stylistic features
  # in translation, especially for creative and literary texts.
  - query: 'large language model translation style coherence'
    provider: "semantic_scholar"
    auto_select_provider: false  # Force Semantic Scholar
    timeframe:
      type: "recent"
      value: "30d"
    max_papers: 20
    extraction_targets:
      - name: "engineering_summary"
        description: "Focus on maintaining style consistency, authorial voice, and coherence in long-form translation"
        output_format: "text"
        required: true

  # -------------------------------------------------------------------------
  # Topic 6: LLM Reasoning (Chain-of-Thought, Tree-of-Thoughts)
  # -------------------------------------------------------------------------
  # Research on reasoning capabilities in LLMs: CoT prompting,
  # Tree-of-Thoughts, self-consistency, reasoning chains.
  - query: 'chain-of-thought reasoning large language model'
    provider: "semantic_scholar"
    auto_select_provider: false
    timeframe:
      type: "recent"
      value: "30d"
    max_papers: 25
    extraction_targets:
      - name: "engineering_summary"
        description: "Focus on reasoning techniques, prompt patterns, and practical applications of CoT/ToT in LLMs"
        output_format: "text"
        required: true

  # -------------------------------------------------------------------------
  # Topic 7: Retrieval-Augmented Generation (RAG)
  # -------------------------------------------------------------------------
  # RAG systems, knowledge grounding, retrieval mechanisms,
  # and hybrid retrieval-generation architectures.
  - query: 'retrieval augmented generation knowledge grounding'
    provider: "semantic_scholar"
    auto_select_provider: false
    timeframe:
      type: "recent"
      value: "30d"
    max_papers: 25
    extraction_targets:
      - name: "engineering_summary"
        description: "Focus on RAG architectures, retrieval strategies, knowledge integration, and practical implementation patterns"
        output_format: "text"
        required: true

  # -------------------------------------------------------------------------
  # Topic 8: LLM Agents and Tool Use
  # -------------------------------------------------------------------------
  # Autonomous LLM agents, function calling, tool use,
  # multi-agent systems, and agentic workflows.
  - query: 'large language model agent tool use function calling'
    provider: "semantic_scholar"
    auto_select_provider: false
    timeframe:
      type: "recent"
      value: "30d"
    max_papers: 25
    extraction_targets:
      - name: "engineering_summary"
        description: "Focus on agent architectures, tool integration patterns, function calling mechanisms, and multi-agent coordination"
        output_format: "text"
        required: true

# =============================================================================
# Global Settings
# =============================================================================
settings:
  # Output configuration
  output_base_dir: "./output"
  enable_duplicate_detection: true

  # Semantic Scholar API Key (loaded from .env)
  semantic_scholar_api_key: "${SEMANTIC_SCHOLAR_API_KEY}"

  # PDF Processing
  # Note: keep_pdfs=false to save disk space for daily runs
  pdf_settings:
    temp_dir: "./temp"
    keep_pdfs: false
    max_file_size_mb: 50
    timeout_seconds: 300

  # LLM Configuration
  # API keys are loaded from .env file
  llm_settings:
    provider: "${LLM_PROVIDER}"
    model: "${LLM_MODEL}"
    api_key: "${LLM_API_KEY}"
    max_tokens: 50000      # Reduced for cost efficiency
    temperature: 0.0       # Deterministic output
    timeout: 300

  # Cost Controls
  # Conservative limits for daily automated runs
  cost_limits:
    max_tokens_per_paper: 50000
    max_daily_spend_usd: 5.0      # Daily safety limit
    max_total_spend_usd: 50.0     # Monthly safety limit

  # Concurrency Settings
  # ULTRA CONSERVATIVE for free tier (5 req/min limit)
  concurrency:
    max_concurrent_downloads: 2
    max_concurrent_conversions: 1
    max_concurrent_llm: 1         # Single LLM call at a time
    queue_size: 20
    checkpoint_interval: 3
    worker_timeout_seconds: 600
    enable_backpressure: true
    backpressure_threshold: 0.8   # Standard threshold

  # Cache Settings
  # Longer TTLs for daily runs to maximize cache hits
  cache:
    enabled: true
    cache_dir: "./cache"
    ttl_api_hours: 24             # 24h for daily runs
    ttl_pdf_days: 7
    ttl_extraction_days: 30

  # Notification Settings (Phase 3.7)
  # Send Slack notifications after each daily run
  notification_settings:
    slack:
      enabled: true               # Enable for daily automated runs
      webhook_url: "${SLACK_WEBHOOK_URL}"
      notify_on_success: true
      notify_on_failure: true
      notify_on_partial: true
      include_cost_summary: true
      include_key_learnings: true
      max_learnings_per_topic: 2
      mention_on_failure: "<!channel>"  # Alert team on failures
